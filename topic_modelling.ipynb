{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling \n",
    "\n",
    "If you have the abstract, suppose you have a 100 papers, you can run topic modelling on the abstract in order to extract the topics in each paper. Then you do this sort of matching. You find a measure of similarity between what a user wants, in order to retrieve the matched papers. (This is called topic modeling: given a bunch of texts, extract its topics.)\n",
    "\n",
    "- Vintage approach: bag of words model. You can start with that. YOu embed the text thanks to the bag of words model. There are many tutorial that show you how to do that.  \n",
    "- Run PCA on the bag of words model! This is called DLA, not PCA. The goal of LDA (latent â€¦ allocation). In python this can be done in 3 lines of code. Simplest approach. \n",
    "- More advanced techniques: Hugging face is a python repository that contains thousands pretrained models. At the heart of large language models (we use them on a daily basis), there is a very peculiar deep learning architecture  which is called transformers. In short, transformers are are the standard for natural language generative models. You can go on hugging face. Hugging face > models > natural language processing (thousands of tasks) > sentence siimilarity/text classification/\n",
    "\n",
    "https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=trending \n",
    "\n",
    "Interesting article: \n",
    "- Leveraging BERTopic for the Analysis of Scientific Papers on Seaweed https://ieeexplore.ieee.org/document/10285737\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# You might need to change the path \n",
    "dionne_path = \"/Users/dionnespaltman/Desktop/Luiss /Data Science in Action/Project/openalex_papers.csv\"\n",
    "df = pd.read_csv(dionne_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you've installed these in your terminal before running the code\n",
    "pip install -U bertopic\n",
    "pip install -U safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic wikipedia\n",
    "Wikipedia BERTopic:  https://huggingface.co/MaartenGr/BERTopic_Wikipedia\n",
    "\n",
    "? Unclear what format the data should be in \n",
    "? Also a relatively small model, so perhaps it's better to use another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the documentation it was used as follows \n",
    "\n",
    "# from bertopic import BERTopic\n",
    "# topic_model = BERTopic.load(\"MaartenGr/BERTopic_Wikipedia\")\n",
    "\n",
    "# topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic.load(df)\n",
    "topic_model.get_topic_info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT base uncased\n",
    "https://huggingface.co/google-bert/bert-base-uncased\n",
    "\n",
    "? Unclear what format the data should be in \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e988a3c369c749c6ba74c79bad5d7ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1d021afeec4108b3dd0ef56fffdac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d86a2a5e6f4a8d8bb08480190b7776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383aab7be1b54710b68f1fc26c5812e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468f3f46617049a2bf32cdec99d5758a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
