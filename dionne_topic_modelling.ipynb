{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling \n",
    "\n",
    "If you have the abstract, suppose you have a 100 papers, you can run topic modelling on the abstract in order to extract the topics in each paper. Then you do this sort of matching. You find a measure of similarity between what a user wants, in order to retrieve the matched papers. (This is called topic modeling: given a bunch of texts, extract its topics.)\n",
    "\n",
    "- Vintage approach: bag of words model. You can start with that. YOu embed the text thanks to the bag of words model. There are many tutorial that show you how to do that.  \n",
    "- Run PCA on the bag of words model! This is called DLA, not PCA. The goal of LDA (latent â€¦ allocation). In python this can be done in 3 lines of code. Simplest approach. \n",
    "- More advanced techniques: Hugging face is a python repository that contains thousands pretrained models. At the heart of large language models (we use them on a daily basis), there is a very peculiar deep learning architecture  which is called transformers. In short, transformers are are the standard for natural language generative models. You can go on hugging face. Hugging face > models > natural language processing (thousands of tasks) > sentence siimilarity/text classification/\n",
    "\n",
    "https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=trending \n",
    "\n",
    "Interesting article: \n",
    "- Leveraging BERTopic for the Analysis of Scientific Papers on Seaweed https://ieeexplore.ieee.org/document/10285737\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to the file\n",
    "json_path = \"/Users/dionnespaltman/Desktop/Luiss /Data Science in Action/Project/openalex_results_clean.json\"\n",
    "\n",
    "# Open and load the JSON data\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame (if it's a list of dicts)\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 'abstract' column as a Pandas Series\n",
    "abstracts = df['abstract']\n",
    "display(abstracts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_list = df['abstract'].tolist()\n",
    "# print(abstracts_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic wikipedia\n",
    "Wikipedia BERTopic:  https://huggingface.co/MaartenGr/BERTopic_Wikipedia\n",
    "\n",
    "? Unclear what format the data should be in \n",
    "? Also a relatively small model, so perhaps it's better to use another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you've installed these in your terminal before running the code\n",
    "# pip install -U bertopic\n",
    "# pip install -U safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertopic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda update numba numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y bertopic\n",
    "!pip install bertopic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a clean df \n",
    "df_clean = df[df['abstract'].notna()].copy()\n",
    "\n",
    "# DataFrame is called df and it has a column 'abstract'\n",
    "docs = df_clean['abstract'].tolist()\n",
    "\n",
    "# Load the pre-trained BERTopic model from Hugging Face\n",
    "topic_model = BERTopic.load(\"MaartenGr/BERTopic_Wikipedia\")\n",
    "\n",
    "# Apply the model to your documents\n",
    "topics, probs = topic_model.transform(docs)\n",
    "\n",
    "# Add results back to your dataframe\n",
    "df_clean['topic_id'] = topics\n",
    "df_clean['topic_label'] = df_clean['topic_id'].apply(\n",
    "    lambda x: topic_model.topic_labels_[x] if x != -1 and x < len(topic_model.topic_labels_) else \"Unknown\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add topic_id and topic_label to the original DataFrame, defaulting to NaN\n",
    "df['topic_id'] = pd.NA\n",
    "df['topic_label'] = pd.NA\n",
    "\n",
    "# Update only the rows that had non-null abstracts\n",
    "df.loc[df['abstract'].notna(), 'topic_id'] = df_clean['topic_id'].values\n",
    "df.loc[df['abstract'].notna(), 'topic_label'] = df_clean['topic_label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the topics \n",
    "display(topic_model.get_topic_info().head())  # Summary of topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize topics \n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic similarity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Work with non-null abstracts\n",
    "df_clean = df[df['abstract'].notna()].copy()\n",
    "\n",
    "# Convert to list\n",
    "docs = df_clean['abstract'].tolist()\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create sentence embeddings\n",
    "embeddings = model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# Add embeddings to the cleaned DataFrame\n",
    "df_clean['embedding'] = list(embeddings)\n",
    "\n",
    "# Add empty column to the original DataFrame\n",
    "df['embedding'] = pd.NA\n",
    "\n",
    "# Merge back into the original DataFrame\n",
    "df.loc[df['abstract'].notna(), 'embedding'] = df_clean['embedding'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute cosine similarity matrix (this compares each doc with every other doc)\n",
    "similarity_matrix = cosine_similarity(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple recommendation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_papers(index, top_n=5):\n",
    "    sim_scores = similarity_matrix[index]\n",
    "    top_indices = np.argsort(sim_scores)[::-1][1:top_n+1]  # skip the paper itself\n",
    "    return df.iloc[top_indices][['title', 'abstract', 'topic_label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_similar_papers(10)  # Recommend similar to paper at index 10\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
