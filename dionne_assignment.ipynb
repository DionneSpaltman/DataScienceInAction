{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science in Action x Deloitte\n",
    "\n",
    "## Questions \n",
    "- How many articles should we roughly get from our query? What query should we use? \n",
    "- Which BERTopic model should we ideally use? Right now, we chose the Wikipedia model, but the model card is very unclear about how to use it. \n",
    "\n",
    "So in order we do (?): \n",
    "1. Download from pyalex (do some data exploration)\n",
    "2. Do the topic modelling to calculate the embeddings (so then you have the vectors). We use only the abstracts for this! \n",
    "3. Semantic similiarity analysis\n",
    "4. Content-based Recommendation system  \n",
    "\n",
    "Options: \n",
    "- Topic Modelling\n",
    "- Text Summarization\n",
    "- Entity Recognition\n",
    "- Content-Based Recommendation System\n",
    "- Semantic Similiarity Analysis\n",
    "- Document Classification \n",
    "\n",
    "\n",
    "## **Objectives of the Project**\n",
    "\n",
    "### **Scientific Papers**\n",
    "- Writers and the universities they are affiliated with.\n",
    "- **Abstract**: The key ingredient for successfully carrying out the project. Abstracts are not subject to strict copyright rules.\n",
    "- **Body of the paper**: The proper content of the paper. **We are not interested in this due to copyright restrictions**.\n",
    "\n",
    "\n",
    "\n",
    "## **Goal: Build a Recommendation System for Scientific Papers**\n",
    "- The objective is to build a **recommendation system (RS)** for academic papers, similar to **Amazon, Spotify, YouTube, or Netflix**, but for scientific research.\n",
    "- Given a paper on a particular topic, the system should **recommend relevant papers**.\n",
    "- The **Objectives Slide** is crucial as it contains all the project goals.\n",
    "\n",
    "### **Understanding the Role of Abstracts**\n",
    "- Abstracts provide a **broad overview** of the research question, methodologies, and results.\n",
    "- **No technical details** are included, making them useful for topic extraction and similarity analysis.\n",
    "\n",
    "### **Topic Discovery**\n",
    "- Using **topic modeling techniques**, we can extract research themes from abstracts.\n",
    "- This enables **matching user queries** with relevant papers.\n",
    "- A typical workflow:\n",
    "  1. Run **topic modeling** on a dataset of abstracts.\n",
    "  2. Extract the key **topics** in each paper.\n",
    "  3. Compute **similarity** between papers to provide recommendations.\n",
    "\n",
    "\n",
    "## **Techniques for Topic Modeling**\n",
    "### **Classic Approach: Bag of Words Model**\n",
    "- A simple way to **embed text** and extract word frequencies.\n",
    "- Apply **LDA (Latent Dirichlet Allocation)** instead of PCA for topic modeling.\n",
    "- Example: LDA in Python can be implemented in **just 3 lines of code**.\n",
    "\n",
    "### **Advanced Approaches**\n",
    "- **Hugging Face** provides thousands of pre-trained models for **natural language processing (NLP)**.\n",
    "- At the core of modern **Large Language Models (LLMs)** is the **Transformer architecture**.\n",
    "- Explore **BERTopic**, a powerful topic modeling tool available on Hugging Face.\n",
    "\n",
    "#### **Hugging Face Resources**\n",
    "- Hugging Face: [https://huggingface.co/](https://huggingface.co/)\n",
    "- BERTopic Model: [https://huggingface.co/MaartenGr/BERTopic_Wikipedia](https://huggingface.co/MaartenGr/BERTopic_Wikipedia)\n",
    "\n",
    "\n",
    "## **Key Considerations for Model Selection**\n",
    "- Some models can be **huge** (e.g., DeepSeek has **685 billion parameters**).\n",
    "- The base **BERT model** has **167 million parameters**.\n",
    "- **More parameters ≠ better accuracy** in practice.\n",
    "- Choose a model **best suited for the task**, rather than one that is too large to run efficiently.\n",
    "\n",
    "\n",
    "## **Alternative Approach: Embeddings for Similarity**\n",
    "- Convert text into **vector representations** using **sentence embeddings**.\n",
    "- **Measure similarity** between abstracts using:\n",
    "  - **Euclidean Distance**\n",
    "  - **Cosine Similarity** (preferred)\n",
    "- Example: Use **Sentence Transformers** from Hugging Face.\n",
    "\n",
    "### **Pros & Cons**\n",
    "- **Embeddings capture meaning**, making them better than **Bag of Words**.\n",
    "- However, **embeddings are not interpretable**, unlike topic modeling.\n",
    "\n",
    "\n",
    "## **Data Collection**\n",
    "### **Why OpenAlex?**\n",
    "- Unlike other datasets, **Deloitte is not providing one**, so we need to **collect our own data**.\n",
    "- **We will use OpenAlex, NOT Semantic Scholar** (Semantic Scholar is not working).\n",
    "- No API key is required for OpenAlex.\n",
    "\n",
    "### **Search Criteria**\n",
    "- Topic: **AI for Pricing and Promotion in GDO**\n",
    "- Time Range: **Last 10 years (2015-2025)**\n",
    "- **Avoid outdated papers!**\n",
    "\n",
    "### **Using OpenAlex API**\n",
    "- API Documentation: [https://docs.openalex.org/api-entities/works](https://docs.openalex.org/api-entities/works)\n",
    "- OpenAlex APIs function **similar to SQL queries**.\n",
    "\n",
    "### **Abstract Handling**\n",
    "- Some papers do not have plaintext abstracts.\n",
    "- Instead, they use **abstract_inverted_index**, which stores **words with their positions**.\n",
    "- This requires **reconstruction**.\n",
    "\n",
    "### **Key Features for Recommendation System**\n",
    "- **Authorships**: Authors are **important indicators** of topic relevance.\n",
    "- **Journals (Sources)**: Papers from similar journals should be ranked higher.\n",
    "- **Keywords & Topics**: OpenAlex assigns **AI-generated keywords** to papers.\n",
    "\n",
    "\n",
    "## **Building the Recommendation System**\n",
    "### **Key Tasks**\n",
    "- **Topic Modeling**: Identify themes using NLP.\n",
    "- **Text Summarization**: (Not necessary since abstracts are already short).\n",
    "- **Entity Recognition**: Extract metadata like author names and institutions.\n",
    "- **Content-Based Recommendation**: Find similar papers based on abstracts.\n",
    "- **Semantic Similarity Analysis**: Compare papers using vector embeddings.\n",
    "\n",
    "### **Suggested NLP Libraries**\n",
    "- **Transformers & Pre-trained models** on Hugging Face.\n",
    "- **Scikit-learn** for similarity evaluation.\n",
    "\n",
    "### **Downloading Papers Efficiently**\n",
    "- **Do not download everything at once** – OpenAlex has a large dataset.\n",
    "- Use **Pyalex** to fetch relevant papers in bulk:\n",
    "  - GitHub Repository: [https://github.com/J535D165/pyalex](https://github.com/J535D165/pyalex)\n",
    "\n",
    "\n",
    "## **Project Explanation (From Slides)**\n",
    "### **Intelligent Analysis for Academic Publications**\n",
    "- The goal is to **revolutionize** the analysis of academic papers using **NLP**.\n",
    "- Manual paper analysis is **slow, inefficient, and unscalable**.\n",
    "- The **proposed solution** improves **speed and accuracy** of information retrieval.\n",
    "\n",
    "### **Project Workflow**\n",
    "1. **Use NLP** to:\n",
    "   - Identify **research themes** (Topic Modeling).\n",
    "   - Recommend **related articles** (Content-based Filtering).\n",
    "   - Summarize abstracts for **quick insights**.\n",
    "\n",
    "### **Deliverables**\n",
    "- **Code**: A well-documented implementation of the recommendation system.\n",
    "- **Technical Report**: Explanation of methodologies, results, and insights.\n",
    "- **(Bonus) Demo Interface**: A user-friendly way to explore results.\n",
    "\n",
    "### **Evaluation Criteria**\n",
    "- **Originality**: Creativity of the solution.\n",
    "- **Business Vision**: How applicable the system is in real-world research.\n",
    "- **Storytelling**: Clear and engaging explanation.\n",
    "- **Graphic Effectiveness**: Presentation quality.\n",
    "- **Technical Soundness**: Performance of the implemented system.\n",
    "\n",
    "\n",
    "## **Next Steps**\n",
    "1. **Finalize the dataset** from OpenAlex.\n",
    "2. **Select NLP techniques** (topic modeling, embeddings).\n",
    "3. **Develop the recommendation system**.\n",
    "4. **Evaluate performance metrics**.\n",
    "\n",
    "\n",
    "## **References**\n",
    "- OpenAlex API: [https://docs.openalex.org/api-entities/works](https://docs.openalex.org/api-entities/works)\n",
    "- Hugging Face Models: [https://huggingface.co/models](https://huggingface.co/models)\n",
    "- Pyalex Library: [https://github.com/J535D165/pyalex](https://github.com/J535D165/pyalex)\n",
    "- Scikit-learn: [https://scikit-learn.org](https://scikit-learn.org)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
